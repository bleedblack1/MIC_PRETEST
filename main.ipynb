{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2559d4-b73b-4be7-9592-ffd98bf47807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d98db6f9-0e30-4b76-bfdd-48cbcf9c54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\"Loads the fine-tuned NER model and tokenizer.\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "        nlp_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "        return tokenizer, nlp_pipeline\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fdbcad7-00be-4d3c-b591-e87ae992f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def clean_text(file_path, cleaned_file_path):\n",
    "    \"\"\"Reads, cleans, and saves text from a file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Remove URLs (http, https, www)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Preserve MIC-related details \"(1)...(2a and 2b)...(3)...\"\n",
    "    pattern_mic_details = r\"\\(\\d+[a-z]?(?: and \\d+[a-z]?)?\\)[^()\\n]+\"\n",
    "    mic_matches = re.findall(pattern_mic_details, text)\n",
    "\n",
    "    # Remove unwanted special characters but keep important ones\n",
    "    text = re.sub(r\"[^\\w\\s.,;()/'\\\"-]\", '', text)\n",
    "\n",
    "    # Ensure MIC-related details remain in their original positions\n",
    "    for match in mic_matches:\n",
    "        if match not in text:\n",
    "            text += \"\\n\" + match  # Appending as a last resort if missing\n",
    "\n",
    "    # Normalize spaces and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).replace(' .', '.').replace(' ,', ',').strip()\n",
    "\n",
    "    # Save cleaned text to a file\n",
    "    with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "    print(f\"✅ Text cleaned and saved to: {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d52f4b71-f906-4876-96f6-3a317bf1328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, nlp_pipeline):\n",
    "    \"\"\"Extracts named entities from the text using the fine-tuned NER model.\"\"\"\n",
    "    ner_results = nlp_pipeline(text)\n",
    "    entities = {\"LABEL_3\": [], \"LABEL_4\": [], \"LABEL_1\": [], \"LABEL_2\": [], \"LABEL_5\": [], \"LABEL_6\": []}\n",
    "    \n",
    "    for entity in ner_results:\n",
    "        label = entity['entity_group']\n",
    "        if label in entities:\n",
    "            entities[label].append(entity['word'])\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cde7ebb5-784b-4e5a-bc53-7a42fd1eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(entities):\n",
    "    \"\"\"Formats extracted entities into structured fields.\"\"\"\n",
    "    dates = \", \".join(entities[\"LABEL_3\"]+entities[\"LABEL_4\"]) or \"N/A\"\n",
    "    \n",
    "    fatalities = sorted(set(entities[\"LABEL_5\"] + entities[\"LABEL_6\"]))\n",
    "    fatality_range = f\"[{fatalities[0]} to {fatalities[-1]}]\" if fatalities else \"N/A\"\n",
    "    \n",
    "    countries = \", \".join(entities[\"LABEL_1\"] + entities[\"LABEL_2\"]) or \"N/A\"\n",
    "    \n",
    "    return dates, fatality_range, countries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d21f339-3a27-40b0-a19c-e3f76e97a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(file_path, model_name, output_csv):\n",
    "    \"\"\"Processes a cleaned text file containing multiple articles and extracts MIC-related entities.\"\"\"\n",
    "    tokenizer, nlp_pipeline = load_model(model_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        articles = file.read().split(\"____________________________________________________________\")\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Article Number\", \"Dates\", \"Fatality Range\", \"Countries\"])\n",
    "        \n",
    "        for idx, article in enumerate(articles, start=1):\n",
    "            entities = extract_entities(article.strip(), nlp_pipeline)\n",
    "            csv_writer.writerow([idx, *format_output(entities)])\n",
    "    \n",
    "    print(f\"✅ Processed {len(articles)} articles. Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb8e5be3-5ffb-4905-9aa4-09aeb76377ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the input text file:  articles/merge/ProQuestDocuments-2025-01-02 (5)_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text cleaned and saved to: cleaned_text.txt\n",
      "✅ Processed 503 articles. Results saved to output_.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = input(\"Enter the path to the input text file: \").strip()\n",
    "    cleaned_file_path = \"cleaned_text.txt\"\n",
    "    output_csv = \"output_.csv\"\n",
    "    model_name = \"roberta_finetuned_MIC_\"\n",
    "\n",
    "    # Step 1: Clean the text and save to a new file\n",
    "    clean_text(file_path, cleaned_file_path)\n",
    "\n",
    "    # Step 2: Process the cleaned text file\n",
    "    process_articles(cleaned_file_path, model_name, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94e538-c60a-4b92-a961-b6670297c0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b07fb2-8376-421e-bd9b-551a5d90782b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01619c3-8371-41bb-9d00-6ea1a70fc8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (newenv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
