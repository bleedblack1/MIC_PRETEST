{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf3488-9627-464c-9ff5-2edfe15eb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Cleans a text file by removing links while keeping MIC-related details.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Remove URLs (http, https, www)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Preserve MIC-related details \"(1)...(2a and 2b)...(3)...\"\n",
    "    pattern_mic_details = r\"\\(\\d+[a-z]?(?: and \\d+[a-z]?)?\\)[^()\\n]+\"\n",
    "    mic_matches = re.findall(pattern_mic_details, text)\n",
    "\n",
    "    # Remove unwanted special characters but keep important ones\n",
    "    text = re.sub(r\"[^\\w\\s.,;()/'\\\"-]\", '', text)\n",
    "\n",
    "    # Ensure MIC-related details remain in their original positions\n",
    "    for match in mic_matches:\n",
    "        if match not in text:\n",
    "            text += \"\\n\" + match  # Appending as a last resort if missing\n",
    "\n",
    "    # Normalize spaces and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).replace(' .', '.').replace(' ,', ',').strip()\n",
    "\n",
    "    # Save cleaned text\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "    \n",
    "    print(f\"✅ Cleaning complete! Saved to: {output_path}\")\n",
    "\n",
    "# Process two files\n",
    "input_files = [\n",
    "    \"articles/merge/ProQuestDocuments-2025-01-02 (5)_3.txt\",\n",
    "]\n",
    "output_files = [\n",
    "    \"cleaned_text_5.txt\",\n",
    "]\n",
    "\n",
    "for inp, out in zip(input_files, output_files):\n",
    "    clean_text(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fe1593-3b7d-44c9-b7b4-1e7cffbecd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BIO-tagged data saved to: train_more_1.json\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.max_length = 5000000  # Increase processing limit if needed\n",
    "\n",
    "# Define entity mappings for BIO tagging\n",
    "ENTITY_MAP = {\n",
    "    \"DATE\": \"DATE\",\n",
    "    \"CARDINAL\": \"FATALITY\",  # Potential fatality numbers\n",
    "    \"GPE\": \"COUNTRY\"\n",
    "}\n",
    "\n",
    "def bio_tagging(text):\n",
    "    \"\"\"\n",
    "    Assigns BIO (Beginning, Inside, Outside) tags to entities in the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens_with_tags = []\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        entity = token.ent_type_\n",
    "        word = token.text\n",
    "        tag = \"O\"  # Default to 'O' (Outside)\n",
    "\n",
    "        # Assign BIO tags based on entity type\n",
    "        if entity in ENTITY_MAP:\n",
    "            label = ENTITY_MAP[entity]\n",
    "            tag = f\"B-{label}\" if not tokens_with_tags or tokens_with_tags[-1][\"label\"] != f\"B-{label}\" else f\"I-{label}\"\n",
    "\n",
    "        # Additional check for fatality indicators\n",
    "        elif entity == \"CARDINAL\" and i < len(doc) - 2:\n",
    "            next_word = doc[i + 1].text.lower()\n",
    "            next_next_word = doc[i + 2].text.lower() if i + 2 < len(doc) else \"\"\n",
    "            if next_word in {\"killed\", \"deaths\", \"fatalities\", \"dead\", \"casualties\"} or \\\n",
    "               next_next_word in {\"killed\", \"deaths\", \"fatalities\", \"dead\", \"casualties\"}:\n",
    "                tag = f\"B-FATALITY\" if not tokens_with_tags or tokens_with_tags[-1][\"label\"] != \"B-FATALITY\" else f\"I-FATALITY\"\n",
    "        \n",
    "        tokens_with_tags.append({\"token\": word, \"label\": tag})\n",
    "\n",
    "    return tokens_with_tags\n",
    "\n",
    "def process_large_text(input_file, output_file, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Processes a large text file in chunks, applies BIO tagging, and saves the output to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    labeled_data = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        for sent in doc.sents:\n",
    "            labeled_data.append({\"sentence\": sent.text, \"tokens\": bio_tagging(sent.text)})\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(labeled_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ BIO-tagged data saved to: {output_file}\")\n",
    "\n",
    "# Example Usage\n",
    "process_large_text(\"cleaned_text_5.txt\", \"train_more_7.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f7aa0-9cc5-4174-a565-4818914ae7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab6ce2-2e17-4153-8040-e4487f2f09df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e94de-2c63-4324-83cd-979c969c123c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9638b-ed20-478e-bafc-df9f51ff6a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac86cc-a34b-400d-8545-df3e5170d052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a478701-6875-4384-93fa-c67ede7368c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tagged data saved to: train_more_2.json\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 5000000  # Increase processing limit if needed\n",
    "\n",
    "# Define entity mappings\n",
    "ENTITY_MAP = {\n",
    "    \"DATE\": \"DATE\",\n",
    "    \"CARDINAL\": \"FATALITY\",  # Potential fatality numbers\n",
    "    \"GPE\": \"COUNTRY\"\n",
    "}\n",
    "\n",
    "def tag_entities(text):\n",
    "    \"\"\"\n",
    "    Tags DATE, FATALITY, and COUNTRY entities in the text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens_with_tags = []\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        entity = token.ent_type_\n",
    "        word = token.text\n",
    "        tag = \"O\"  # Default to 'O' (Outside)\n",
    "\n",
    "        if entity in ENTITY_MAP:\n",
    "            label = ENTITY_MAP[entity]\n",
    "            tag = label\n",
    "        \n",
    "        # Additional check for fatality indicators\n",
    "        elif entity == \"CARDINAL\" and i < len(doc) - 2:\n",
    "            next_word = doc[i + 1].text.lower()\n",
    "            next_next_word = doc[i + 2].text.lower() if i + 2 < len(doc) else \"\"\n",
    "            if next_word in {\"killed\", \"deaths\", \"fatalities\", \"dead\", \"casualties\"} or \\\n",
    "               next_next_word in {\"killed\", \"deaths\", \"fatalities\", \"dead\", \"casualties\"}:\n",
    "                tag = \"FATALITY\"\n",
    "        \n",
    "        tokens_with_tags.append({\"token\": word, \"label\": tag})\n",
    "\n",
    "    return tokens_with_tags\n",
    "\n",
    "def process_large_text(input_file, output_file, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Processes a large text file in chunks, applies entity tagging, and saves the output to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    labeled_data = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        for sent in doc.sents:\n",
    "            labeled_data.append({\"sentence\": sent.text, \"tokens\": tag_entities(sent.text)})\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(labeled_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Tagged data saved to: {output_file}\")\n",
    "\n",
    "# Example Usage\n",
    "process_large_text(\"cleaned_text_5.txt\", \"train_more_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d1840-e175-4e44-b52f-138462771b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (newenv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
